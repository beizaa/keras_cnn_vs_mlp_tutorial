{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification - From MLPs to ConvNets\n",
    "\n",
    "For this practical we are going to train a small network for digit classification on images. \n",
    "\n",
    "First, we are going to train an MLP classifier, using only Dense layers (and dropout). \n",
    "\n",
    "Next, we are going to replace the MLP with a ConvNet, using Convolutional Layers, Max Pooling etc. \n",
    "\n",
    "Your task is to try variations of the above and compare their performance. Do pay attention to the order of layers in ConvNets and their importance in performance increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras makes it very easy to use the MNIST dataset by simply loading it. Explore the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the 47187th sample with label 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16e41a7ccc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e3db398d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import random\n",
    "\n",
    "n_th_image = random.randint(0, x_train.shape[0])  # We print a random image every time\n",
    "print(f\"Printing the {n_th_image}th sample with label {y_train[n_th_image]}\")\n",
    "plt.imshow(x_train[n_th_image], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some technicalities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dropout(0.2))\n",
    "mlp_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sindi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.7864 - accuracy: 0.7620 - val_loss: 0.3197 - val_accuracy: 0.9079\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.3306 - accuracy: 0.9042 - val_loss: 0.2400 - val_accuracy: 0.9284\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2628 - accuracy: 0.9239 - val_loss: 0.1991 - val_accuracy: 0.9411\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2237 - accuracy: 0.9344 - val_loss: 0.1773 - val_accuracy: 0.9468\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1980 - accuracy: 0.9434 - val_loss: 0.1586 - val_accuracy: 0.9524\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1769 - accuracy: 0.9481 - val_loss: 0.1582 - val_accuracy: 0.9519\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1611 - accuracy: 0.9533 - val_loss: 0.1389 - val_accuracy: 0.9582\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1483 - accuracy: 0.9567 - val_loss: 0.1312 - val_accuracy: 0.9597\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1352 - accuracy: 0.9606 - val_loss: 0.1201 - val_accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1256 - accuracy: 0.9635 - val_loss: 0.1148 - val_accuracy: 0.9654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16e3fa59e48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1: Build your own MLP model\n",
    "Try to use different amount of dense layers, variations of dropout etc. Report your architecture with the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# my_mlp_model = Sequential()\n",
    "# my_mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "###### YOUR LAYERS HERE\n",
    "\n",
    "# my_mlp_model.add(Dense(??, activation=??))\n",
    "# my_mlp_model.add(Dropout(??))\n",
    "\n",
    "############\n",
    "# my_mlp_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# my_mlp_model.compile(optimizer='sgd', \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "# my_mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "conv_model = Sequential()\n",
    "conv_model.add(Conv2D(16, kernel_size=(3,3)))\n",
    "conv_model.add(BatchNormalization())\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(Conv2D(16, kernel_size=(3,3),activation='relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "conv_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.2029 - accuracy: 0.9397 - val_loss: 0.0829 - val_accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0787 - accuracy: 0.9762 - val_loss: 0.0690 - val_accuracy: 0.9784\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0609 - accuracy: 0.9815 - val_loss: 0.0536 - val_accuracy: 0.9812\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.0519 - accuracy: 0.9839 - val_loss: 0.0486 - val_accuracy: 0.9842\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0450 - accuracy: 0.9863 - val_loss: 0.0509 - val_accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16e52426940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "conv_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Build your own ConvNet\n",
    "Try different variations of layers, more convolutions, with/without dropout, more/less fully connected layer at the end. Report your best result. \n",
    "\n",
    "Optional: Can you build it without fully connected layers at the end? (Hint: Use number of channels in the output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# my_conv_model = Sequential()\n",
    "\n",
    "###### YOUR LAYERS HERE ######\n",
    "# my_conv_model.add(Conv2D(??, kernel_size=(?,?)))\n",
    "# my_conv_model.add(BatchNormalization())\n",
    "# my_conv_model.add(MaxPooling2D(pool_size=(? ?)))\n",
    "\n",
    "##############################\n",
    "\n",
    "# my_conv_model.compile(optimizer='sgd', \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "# my_conv_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADONJREFUeJzt3X+oXPWZx/HPR7dBSQsqGWOwurdbVFaFpjLEhSyLS68lXSuxYqURYlZKbwMJWKyw4j8JhhVdtnarrIF0E5tKaxtp1fyhbiUsZAtL8Sra2I3ai9xtY2Jywy0kRaTG++wf96Rc451zJ3POzJn4vF8QZuY858fD6Oeemfmema8jQgDyOavpBgA0g/ADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0jqLwZ5sCVLlsTIyMggDwmkMjk5qaNHj7qbdSuF3/YqSd+TdLak/4iIB8rWHxkZ0fj4eJVDAijRbre7Xrfnl/22z5b075K+JOlKSWtsX9nr/gAMVpX3/CskTUTEWxHxJ0k/kbS6nrYA9FuV8F8s6fdzHh8oln2I7THb47bHp6amKhwOQJ2qhH++DxU+8v3giNgWEe2IaLdarQqHA1CnKuE/IOmSOY8/LelgtXYADEqV8L8o6TLbn7G9SNLXJO2upy0A/dbzUF9EnLC9UdJ/anaob0dE/Ka2zgD0VaVx/oh4VtKzNfUCYIC4vBdIivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAY6RTcwSNPT0x1ro6OjpduOjY2V1tevX99TT8OEMz+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJFVpnN/2pKTjkj6QdCIi2nU0BUjSsWPHSus7d+4srT/22GMda6+//nrptkuWLCmtfxzUcZHP30fE0Rr2A2CAeNkPJFU1/CHpF7Zfsl1+PSSAoVL1Zf/KiDho+0JJL9h+PSL2zl2h+KMwJkmXXnppxcMBqEulM39EHCxuj0h6StKKedbZFhHtiGi3Wq0qhwNQo57Db3ux7U+dvC/pi5Jeq6sxAP1V5WX/UklP2T65nx9HxPO1dAWg73oOf0S8JelzNfYCfMi1115bWp+YmCitz8zMdKxdccUVpdvecsstpfWPA4b6gKQIP5AU4QeSIvxAUoQfSIrwA0nx093oqxMnTnSsPfjgg6XbTk5OVjr2ww8/3LF2++23V9r3xwFnfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IinF+9NXWrVs71jZv3tzXY2/YsKGv+z/TceYHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQY50cl09PTpfWyabLLflpbkm6++ebS+pNPPllaRznO/EBShB9IivADSRF+ICnCDyRF+IGkCD+Q1ILj/LZ3SPqypCMRcXWx7AJJP5U0ImlS0q0R8Yf+tYmmvPvuu6X10dHR0vq+ffs61lqtVum2W7ZsKa2jmm7O/D+QtOqUZfdI2hMRl0naUzwGcAZZMPwRsVfSqZdxrZa0s7i/U9JNNfcFoM96fc+/NCIOSVJxe2F9LQEYhL5/4Gd7zPa47fGpqal+Hw5Al3oN/2HbyySpuD3SacWI2BYR7YhoL/QBD4DB6TX8uyWtK+6vk/RMPe0AGJQFw2/7CUn/I+kK2wdsf13SA5Kut/1bSdcXjwGcQRYc54+INR1KX6i5FwyhTZs2ldZfffXV0nrZW729e/eWbnv55ZeX1lENV/gBSRF+ICnCDyRF+IGkCD+QFOEHkuKnu5Pbs2dPaX379u2V9r979+6ONYbymsWZH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSYpw/uY0bN5bWjx8/Xlq/8cYbS+tXXXXVafeEweDMDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJMc7/Mffoo4+W1t98881K+1+/fn1pffHixZX2j/7hzA8kRfiBpAg/kBThB5Ii/EBShB9IivADSS04zm97h6QvSzoSEVcXyzZL+oakqWK1eyPi2X41iXLvvPNOx9ojjzxSuu1ZZ5X//V+6dGlpfdWqVaV1DK9uzvw/kDTff+HvRsTy4h/BB84wC4Y/IvZKmh5ALwAGqMp7/o22f217h+3za+sIwED0Gv6tkj4rabmkQ5K+02lF22O2x22PT01NdVoNwID1FP6IOBwRH0TEjKTvS1pRsu62iGhHRLvVavXaJ4Ca9RR+28vmPPyKpNfqaQfAoHQz1PeEpOskLbF9QNImSdfZXi4pJE1K+mYfewTQBwuGPyLWzLO42qTtOC1vv/12aX10dLRjbWJiotKxH3/88UrbY3hxhR+QFOEHkiL8QFKEH0iK8ANJEX4gKX66+wywa9eu0nqV4by77rqrtL5y5cqe943hxpkfSIrwA0kRfiApwg8kRfiBpAg/kBThB5JinH8IPP/886X1u+++u7Re9vPbd9xxR+m2W7ZsKa0vWrSotI4zF2d+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKcf4BeOONN0rrN9xwQ2l9ZmamtH7nnXd2rD300EOl2yIvzvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kNSC4/y2L5H0Q0kXSZqRtC0ivmf7Akk/lTQiaVLSrRHxh/61Orzee++90vqmTZtK62Xfx5ek2267rbR+//33l9aB+XRz5j8h6dsR8deS/kbSBttXSrpH0p6IuEzSnuIxgDPEguGPiEMR8XJx/7ik/ZIulrRa0s5itZ2SbupXkwDqd1rv+W2PSPq8pF9JWhoRh6TZPxCSLqy7OQD903X4bX9S0s8kfSsijp3GdmO2x22PT01N9dIjgD7oKvy2P6HZ4P8oIn5eLD5se1lRXybpyHzbRsS2iGhHRLvVatXRM4AaLBh+25a0XdL+iJj7FbHdktYV99dJeqb+9gD0Szdf6V0paa2kfbZfKZbdK+kBSbtsf13S7yR9tT8tDoey4by1a9eWbvv0009XOvZFF11UWj/nnHMq7R85LRj+iPilJHcof6HedgAMClf4AUkRfiApwg8kRfiBpAg/kBThB5Lip7u7VDbOX3Uc/5prrimt33fffZX2D8yHMz+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJMU4/wAsNI7/3HPPldbPPffcOtsBJHHmB9Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOfv0nnnndex9v777w+wE6AenPmBpAg/kBThB5Ii/EBShB9IivADSRF+IKkFw2/7Etv/ZXu/7d/YvrNYvtn227ZfKf79Q//bBVCXbi7yOSHp2xHxsu1PSXrJ9gtF7bsR8a/9aw9AvywY/og4JOlQcf+47f2SLu53YwD667Te89sekfR5Sb8qFm20/WvbO2yf32GbMdvjtsenpqYqNQugPl2H3/YnJf1M0rci4pikrZI+K2m5Zl8ZfGe+7SJiW0S0I6LdarVqaBlAHboKv+1PaDb4P4qIn0tSRByOiA8iYkbS9yWt6F+bAOrWzaf9lrRd0v6IeGjO8mVzVvuKpNfqbw9Av3Tzaf9KSWsl7bP9SrHsXklrbC+XFJImJX2zLx0C6ItuPu3/pSTPU3q2/nYADApX+AFJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5JyRAzuYPaUpP+bs2iJpKMDa+D0DGtvw9qXRG+9qrO3v4yIrn4vb6Dh/8jB7fGIaDfWQIlh7W1Y+5LorVdN9cbLfiApwg8k1XT4tzV8/DLD2tuw9iXRW68a6a3R9/wAmtP0mR9AQxoJv+1Vtt+wPWH7niZ66MT2pO19xczD4w33ssP2EduvzVl2ge0XbP+2uJ13mrSGehuKmZtLZpZu9LkbthmvB/6y3/bZkt6UdL2kA5JelLQmIv53oI10YHtSUjsiGh8Ttv13kv4o6YcRcXWx7F8kTUfEA8UfzvMj4p+GpLfNkv7Y9MzNxYQyy+bOLC3pJkn/qAafu5K+blUDz1sTZ/4VkiYi4q2I+JOkn0ha3UAfQy8i9kqaPmXxakk7i/s7Nfs/z8B16G0oRMShiHi5uH9c0smZpRt97kr6akQT4b9Y0u/nPD6g4ZryOyT9wvZLtseabmYeS4tp009On35hw/2casGZmwfplJmlh+a562XG67o1Ef75Zv8ZpiGHlRFxjaQvSdpQvLxFd7qauXlQ5plZeij0OuN13ZoI/wFJl8x5/GlJBxvoY14RcbC4PSLpKQ3f7MOHT06SWtweabifPxummZvnm1laQ/DcDdOM102E/0VJl9n+jO1Fkr4maXcDfXyE7cXFBzGyvVjSFzV8sw/vlrSuuL9O0jMN9vIhwzJzc6eZpdXwczdsM143cpFPMZTxb5LOlrQjIv554E3Mw/ZfafZsL81OYvrjJnuz/YSk6zT7ra/DkjZJelrSLkmXSvqdpK9GxMA/eOvQ23Wafen655mbT77HHnBvfyvpvyXtkzRTLL5Xs++vG3vuSvpaowaeN67wA5LiCj8gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0n9P7I3or700J6DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e50a42828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = conv_model # SET YOUR MODEL HERE (mlp_model, conv_model, my_mlp_model or my_conv_model)\n",
    "random_index = random.randint(0, x_test.shape[0])\n",
    "plt.imshow(x_test[random_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[random_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3 (optional): Try different normalization techniques and compare results with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4 (optional): Try different optimizers (ADAM, RMSProp etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 5 (optional): Compare execution time for 10 epochs of an MLP and a ConvNet with the same number of layers. \n",
    "Which one is faster? How does this relate to the number of parameters? What about the computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a list of conclusions here: \n",
    "\n",
    "E.g: Dropout is useless in ConvNets, BatchNorm is good etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
